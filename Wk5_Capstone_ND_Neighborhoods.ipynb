{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge BeautifulSoup4 --yes \n",
    "# !conda install -c conda-forge requests --yes \n",
    "# !conda install -c conda-forge lxml --yes \n",
    "# !conda install -c conda-forge html5lib --yes \n",
    "# !conda install -c conda-forge geopy --yes\n",
    "# !conda install -c conda-forge geocoder --yes\n",
    "\n",
    "print('all installed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # for webscarping\n",
    "import requests #to call the link\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for maps\n",
    "import folium\n",
    "\n",
    "import geocoder\n",
    "\n",
    "#for json files\n",
    "import json\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#import for the lat and lang as the geocoder is not working\n",
    "from geopy.geocoders import Nominatim\n",
    "print('All imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Beautiful Soup to parse the link after reading it using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_IN = 'https://raw.githubusercontent.com/sanand0/pincode/master/data/IN.csv'\n",
    "#source =  requests.get ('https://raw.githubusercontent.com/sanand0/pincode/master/data/IN.csv')\n",
    "#soup  = BeautifulSoup(source,'csv')\n",
    "#print (soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting all needed tables from the link in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url_IN)\n",
    "data = data.rename(columns={'key' :'Zip','place_name':'District','admin_name1':'State','latitude':'Latitude','longitude':'Longitude'})\n",
    "#filter for New Delhi\n",
    "df_data = data[data['State'].str.contains('New Delhi')].reset_index(drop=True)\n",
    "df_data = df_data.drop(columns=['accuracy','Zip'],axis=1)\n",
    "df_data = df_data.sort_values('District',ascending = False).groupby(['Latitude','Longitude']).head(1).reset_index(drop=True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the final dataset as per requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final data has {} rows'.format(df_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing New Delhi in the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First checking co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'New Delhi, India'\n",
    "geolocator = Nominatim(user_agent=\"IN_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of central New Delhi are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = address\n",
    "geoloc = Nominatim(user_agent='P')\n",
    "#print(geoloc)\n",
    "print(geoloc.geocode(add))\n",
    "loc = geoloc.geocode(add)\n",
    "lat = loc.latitude \n",
    "lng = loc.longitude\n",
    "print(loc.latitude, loc.longitude)\n",
    "label = 'ND'\n",
    "map_dt1 =  folium.Map(location=[loc.latitude,loc.longitude], zoom_start=11)\n",
    "#map_dt1\n",
    "label = folium.Popup(label, parse_html= True)\n",
    "folium.CircleMarker( \n",
    "    [lat,lng], \n",
    "        radius=3, \n",
    "        popup= label,\n",
    "        color = 'green',\n",
    "        fill = True,\n",
    "        fill_color = '#33FF4F',\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html =False).add_to(map_dt1)\n",
    "#map_dt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now checking the areas of Delhi from the dataframe table on the map marking the Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map creation\n",
    "map_DT = folium.Map(location=[latitude,longitude],zoom_start=10)\n",
    "\n",
    "#adding markers on above map\n",
    "\n",
    "for lat, lng, label in zip(df_data['Latitude'],df_data['Longitude'],df_data['District']) :\n",
    "    label = folium.Popup(label, parse_html= True)\n",
    "    folium.CircleMarker( \n",
    "        [lat,lng], \n",
    "        radius=3, \n",
    "        popup= label,\n",
    "        color = 'green',\n",
    "        fill = True,\n",
    "        fill_color = '#33FF4F',\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html =False).add_to(map_DT)\n",
    "map_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the neighborhoods around Downtown Toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using existing Foursquare creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'ANUSTZNTX5MOJM10FOMOTATTFTLZ2C4WVMAGBCTTRCWQZEOO' # your Foursquare ID\n",
    "CLIENT_SECRET = 'V4PHMEO5TGQ22DL1KYFB4TFDKX4N4SQ4AS5XOXBSLM0I4Z1W' # your Foursquare Secret\n",
    "VERSION = '20180628' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking any one of the neighborhoods to explore first - Connaught Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[14,'District']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's shorten Connought Place as CP for upcoming analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_name = df_data.loc[14,'District']\n",
    "CP_lat  =  df_data.loc[14,'Latitude']\n",
    "CP_lng  = df_data.loc[14,'Longitude']\n",
    "print('Location credentials for CP : Name {}, Lat {}, Long {}'.format(CP_name, CP_lat, CP_lng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting url for exploration using Foursquare API and limiting the results to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 10000\n",
    "LIMIT = 50\n",
    "url =  'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, CP_lat, CP_lng, VERSION, radius, LIMIT)\n",
    "#url_hosp =  'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&categoryID={}&v={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, HF_lat, HF_lng,categoryID, VERSION, radius, LIMIT)\n",
    "print(url)\n",
    "results = requests.get(url).json()#['response']['venues']\n",
    "#print(url_hosp)\n",
    "#results_hosp = requests.get(url_hosp).json()#['response']['venues']\n",
    "#results_hosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only taking the required values\n",
    "venues = results['response']['venues']\n",
    "print (venues)\n",
    "#flatten the json\n",
    "\n",
    "df_CP = json_normalize(venues)\n",
    "df_CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = ['name','categories','location.lat','location.lng']\n",
    "filtered_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leveraging the get category type function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_type(i):\n",
    "    try :\n",
    "        cateogories =  i['venue.categories']\n",
    "    except :\n",
    "        categories =  i['categories']\n",
    "        \n",
    "        if len(categories) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return categories[0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the json and loading in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "CP_venues = df_CP.loc[:, filtered_columns]\n",
    "CP_venues['categories'] = CP_venues.apply(get_category_type,axis=1)\n",
    "                  \n",
    "CP_venues.columns= [col.split(\".\")[-1] for col in CP_venues.columns]    \n",
    "CP_venues=CP_venues[CP_venues['categories'].notna()]\n",
    "CP_venues['categories'].unique()\n",
    "## exporting the data in case the foursquari API is evoked for 24 hours\n",
    "CP_venues.to_csv(\"CP_Venues.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rows returned by Foursquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_venues.categories.unique()\n",
    "# #LIMIT == HF_venues.shape[0]\n",
    "# HF_venues.groupby('categories').count().sort_values('name',ascending=False)\n",
    "# HF_venues=HF_venues[HF_venues['categories'].notna()]\n",
    "# d = HF_venues[HF_venues['categories'].str.contains('')]#('Restaurant|Pub|Caf|Bar|Snack|Coffee|Juic')]\n",
    "# HF_venues.groupby('categories').count().sort_values('name',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limit that we set matches with the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now to replicate the process for all other areas of ND - we will create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, lats, lngs, radius = 10000):\n",
    "    \n",
    "    venues_list = []\n",
    "    for name, lat, lng in zip(names, lats, lngs):\n",
    "        #print(name) # to get the names of the venues after callign the function\n",
    "        \n",
    "        #using the similar method as for Harbourfront\n",
    "        \n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "        #print(url)\n",
    "        results = requests.get(url).json()['response']['groups'][0]['items']\n",
    "        #print (requests.get(url))\n",
    "        \n",
    "        venues_list.append([(\n",
    "            name,\n",
    "            lat,\n",
    "            lng,\n",
    "            v['venue']['name'],\n",
    "            v['venue']['location']['lat'],\n",
    "            v['venue']['location']['lng'],\n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "        \n",
    "        nearby_ven = pd.DataFrame([item for venue_list in venues_list for item in venue_list ])\n",
    "        nearby_ven.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "\n",
    "        \n",
    "    return(nearby_ven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_venues = getNearbyVenues(names=df_data['District'],\n",
    "                                 lats= df_data['Latitude'],\n",
    "                                 lngs =df_data['Longitude'], radius = 500\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_venues.shape\n",
    "#print (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_venues.to_csv(\"ND_venues.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking number of venues for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_venues.groupby('Neighborhood').count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Queen's Park is not coming as we were not able to retrieve any data from it's url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(ND_venues['Venue Category'].unique())))\n",
    "ND_venues['Venue Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing each neighborhood before clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing onehot encoding\n",
    "\n",
    "ND_onehot =  pd.get_dummies(ND_venues['Venue Category'])\n",
    "\n",
    "# rename of column required as the Venue category value is also 'Neighborhood'\n",
    "ND_onehot = ND_onehot.rename(columns={\"Neighborhood\":\"Neighboring\"})\n",
    "\n",
    "ND_onehot = pd.concat([ND_venues['Neighborhood'],ND_onehot],axis=1)\n",
    "ND_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding for frequency calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_grouped = ND_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "#d = ND_grouped.iloc[0,:]\n",
    "#downtown_grouped\n",
    "#print(d.iloc[1:].sort_values(ascending=False))\n",
    "#d.iloc[1:].sort_values(ascending=False).index.values[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking top venues in the neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_venue_num = 10\n",
    "\n",
    "for hood in ND_grouped['Neighborhood']:\n",
    "   \n",
    "    #selecting each row and transposing to get frequency of venues for each neighborhood\n",
    "    temp = ND_grouped[ND_grouped['Neighborhood']==hood].T.reset_index()\n",
    "    \n",
    "    #selecting from 2nd row as the first row has neighborhood values\n",
    "    temp = temp.iloc[1:]\n",
    "    \n",
    "    #defining columns for the temp table\n",
    "    temp.columns=['Venue','Freq']\n",
    "    #print(hood)\n",
    "    #printing top 2 venues for each neighborhood\n",
    "    #print(temp.sort_values('Freq',ascending = False).head(top_venue_num).reset_index(drop=True))\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to get top venues for each neighborhood and storing them in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_venues(row, num_top):\n",
    "    row_categ_top = row.iloc[1:].sort_values(ascending=False)\n",
    "    return row_categ_top.index.values[0:num_top]\n",
    "\n",
    "#downtown_grouped.iloc[1, 1:]\n",
    "#get_top_venues(downtown_grouped.iloc[1,1:], 5)\n",
    "ND_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the dataframe now for all neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top = 10  #to get top venues\n",
    "columns = ['Neighborhood']\n",
    "indicators = ['st', 'nd', 'rd'] #for column names as they will append to 1,2,3 to make them 1st, 2nd, 3rd\n",
    "for i in range(1, num_top):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(i, indicators[i-1]))\n",
    "    except:     \n",
    "        columns.append('{}th Most common venue'.format(i))\n",
    "\n",
    "neighborhood_venues_top = pd.DataFrame(columns=columns) #adding the columns to the dataframe\n",
    "neighborhood_venues_top['Neighborhood'] = ND_grouped['Neighborhood'] #copying neighborhood data to the new dataframe\n",
    "\n",
    "for j in range(0, ND_grouped.shape[0]):\n",
    "    neighborhood_venues_top.iloc[j, 1:] = get_top_venues(ND_grouped.iloc[j, :], num_top-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_venues_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusters = 6 # for number of clusters\n",
    "\n",
    "downtown_clustering =  ND_grouped.drop('Neighborhood',axis=1) #dropping the column Neighborhood as it won't be needed for clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(downtown_clustering)\n",
    "kmeans.labels_[0:142] #checking the cluster groups for each neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging two datasets to get additional fields along with clusters for the neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_venues_top.insert(0,'Cluster Labels',kmeans.labels_) #inserting the column Cluster Labels\n",
    "df_data = df_data.rename(columns={'District':'Neighborhood'})\n",
    "#df_data\n",
    "merged_data = pd.merge(df_data,neighborhood_venues_top,how='inner',on='Neighborhood') #joining on Neighborhood column values\n",
    "#merged_downtown_dat = merged_downtown_data[merged_downtown_data['Cluster Labels' == 0]]\n",
    "merged_data.head(5)\n",
    "#merged_downtown_data = merged_downtown_data[merged_downtown_data['Cluster Labels'] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeing the clustering on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=6)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(merged_data['Latitude'], merged_data['Longitude'], merged_data['Neighborhood'], merged_data['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=3,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can check the data for each cluster using below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_merged =  merged_data\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==0, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==1, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==2, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==3, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==4, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downtown_merged.loc[downtown_merged['Cluster Labels']==5, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]].sort_values('Cluster Labels').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
